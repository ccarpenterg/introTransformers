Introduction to Transformers
============================

### 1a) Introduction to Attention and Transformers

Notebook: ([Jupyter notebook](https://nbviewer.jupyter.org/github/ccarpenterg/introTransformers/blob/master/01a_introduction_NLP_transformers.ipynb)) ([github](https://github.com/ccarpenterg/introTransformers/blob/master/01a_introduction_NLP_transformers.ipynb))

## References

**Lectures and Presentations**

[1] Machine Translation, Seq2Seq and Attention ([slides](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture08-nmt.pdf)) ([video](https://youtu.be/XXtpJxZBa2c)). CS224n: NLP with Deep Learning (2019), Stanford University

[2] Modeling contexts of use: Contextual Representations and Pretraining ([slides](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture13-contextual-representations.pdf)) ([video](https://youtu.be/S-CspeZ8FHc)). CS224n: NLP with Deep Learning (2019), Stanford University

[3] Transformers and Self-Attention For Generative Models ([slides](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture14-transformers.pdf)) ([video](https://youtu.be/5vcj8kSwBCY)). Guest lectcure by Ashish Vaswani and Anna Huang.

**NLP Books**

[4] Speech and Language Processing ([3rd ed. draft](https://web.stanford.edu/~jurafsky/slp3/)). Dan Jurafsky, James H. Martin

[5] Dive into Deep Learning ([online version](https://d2l.ai/)). Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola

[6] Deep Learning ([online version](https://www.deeplearningbook.org/)). Ian Goodfellow, Yoshua Bengio, AAron Courville

**Papers**

[7] Neural Machine Translation by Jointly Learning to Align and Translate. [Bahdanau 2016](https://arxiv.org/pdf/1409.0473.pdf)

[8] Attention is All You Need, [Vaswani 2017](https://arxiv.org/pdf/1706.03762.pdf).
